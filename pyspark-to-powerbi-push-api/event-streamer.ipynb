{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from faker import Faker\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EcommerceStreaming\").getOrCreate()\n",
    "\n",
    "# Define the schema for the dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Create the Faker instance\n",
    "fake = Faker()\n",
    "\n",
    "# Define the products list\n",
    "products = ['book', 'phone', 'laptop', 'tablet', 'smartwatch']\n",
    "\n",
    "def simulate_purchase():\n",
    "    # Generate the data for each purchase\n",
    "    customer_id = fake.uuid4()\n",
    "    product = random.choice(products)\n",
    "    quantity = random.randint(1, 10)\n",
    "    price = random.uniform(100, 1000)\n",
    "    date = datetime.datetime.now()\n",
    "\n",
    "    return (customer_id, product, quantity, price, date)\n",
    "\n",
    "# Start the streaming process\n",
    "while True:\n",
    "    # Set a random batch size between 1 and 10\n",
    "    batch = range(random.randint(1,10))\n",
    "    # Generate the data for the current batch\n",
    "    data = [simulate_purchase() for i in batch]\n",
    "\n",
    "    # Convert the data to a dataframe and add it to the stream\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "    df = df.withColumn(\"timestamp\", from_unixtime(unix_timestamp(col(\"date\"))).cast(TimestampType()))\n",
    "    df.show()\n",
    "\n",
    "    # Wait for 1 seconds before the next batch\n",
    "    time.sleep(1)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
